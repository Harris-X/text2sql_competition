#!/usr/bin/env python3
"""Batch predictor for the Qwen3-Coder-30B-A3B-Instruct LoRA adapter on Spider/MySQL data."""

import argparse
import json
from pathlib import Path
from typing import List, Optional

import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer


def build_prompt(sample: dict) -> str:
    instruction = sample.get("instruction", "").strip()
    input_text = sample.get("input", "").strip()
    if instruction and input_text:
        return f"{instruction}\n\n{input_text}"
    if instruction:
        return instruction
    return input_text


def load_metadata(meta_path: Optional[Path]) -> Optional[List[dict]]:
    if meta_path is None:
        return None
    if not meta_path.exists():
        raise FileNotFoundError(f"Metadata file not found: {meta_path}")
    with meta_path.open("r", encoding="utf-8") as file:
        return [json.loads(line) for line in file if line.strip()]


def sanitize_instance_id(value: str) -> str:
    safe = value.replace("/", "_").replace("\\", "_")
    return safe.strip() or "sample"


def write_sql_file(output_dir: Path, instance_id: str, content: str) -> None:
    output_dir.mkdir(parents=True, exist_ok=True)
    file_path = output_dir / f"{instance_id}.sql"
    text = content.rstrip() + "\n"
    file_path.write_text(text, encoding="utf-8")


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Generate SQL predictions with the Qwen3 LoRA adapter and optionally export submission files."
    )
    parser.add_argument(
        "--base-model",
        default="/root/autodl-tmp/comp/LLaMA-Factory/Qwen3-Coder-30B-A3B-Instruct",
        help="Path to the base Qwen3-Coder-30B-A3B-Instruct model directory.",
    )
    parser.add_argument(
        "--adapter-path",
        default="/root/autodl-tmp/comp/LLaMA-Factory/saves/qwen3-coder-30b/lora/mysql-sft",
        help="Path to the trained LoRA adapter directory (can also be a checkpoint subfolder).",
    )
    parser.add_argument(
        "--dataset",
        default="/root/autodl-tmp/comp/LLaMA-Factory/data/qwen3_sft_dataset_alpaca.json",
        help="Path to the Alpaca-formatted SFT dataset used for training.",
    )
    parser.add_argument(
        "--metadata",
        default="/root/autodl-tmp/comp/LLaMA-Factory/Spider2/spider2-snow/qwen3_sft_dataset.jsonl",
        help="Optional JSONL file containing instance_id/question/output metadata (generated by text2sql.py).",
    )
    parser.add_argument(
        "--output-dir",
        default="/root/autodl-tmp/comp/LLaMA-Factory/Spider2/spider2-snow/evaluation_suite/qwen3_lora_predictions",
        help="Directory to write per-instance .sql files following the evaluation README. Set to '' to disable writing.",
    )
    parser.add_argument(
        "--results-jsonl",
        default="/root/autodl-tmp/comp/LLaMA-Factory/saves/qwen3-coder-30b/lora/mysql-sft/predictions.jsonl",
        help="Where to dump aggregated predictions as JSONL. Set to '' to skip saving the summary.",
    )
    parser.add_argument(
        "--num-samples",
        type=int,
        default=10,
        help="How many samples to evaluate (0 means the entire dataset).",
    )
    parser.add_argument(
        "--preview",
        type=int,
        default=3,
        help="Log the first N predictions to stdout for inspection.",
    )
    parser.add_argument(
        "--max-new-tokens",
        type=int,
    default=65536,
        help="Maximum number of new tokens to generate for each prompt.",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.1,
        help="Sampling temperature; set to 0 for greedy decoding.",
    )
    parser.add_argument(
        "--top-p",
        type=float,
        default=0.7,
        help="Top-p for nucleus sampling; ignored when temperature is 0.",
    )
    args = parser.parse_args()

    base_model_path = Path(args.base_model).expanduser().resolve()
    adapter_path = Path(args.adapter_path).expanduser().resolve()
    dataset_path = Path(args.dataset).expanduser().resolve()
    metadata_path = Path(args.metadata).expanduser().resolve() if args.metadata else None
    output_dir = Path(args.output_dir).expanduser().resolve() if args.output_dir else None
    results_jsonl = Path(args.results_jsonl).expanduser().resolve() if args.results_jsonl else None

    if not base_model_path.exists():
        raise FileNotFoundError(f"Base model path not found: {base_model_path}")
    if not adapter_path.exists():
        raise FileNotFoundError(f"Adapter path not found: {adapter_path}")
    if not dataset_path.exists():
        raise FileNotFoundError(f"Dataset path not found: {dataset_path}")
    if metadata_path is not None and not metadata_path.exists():
        raise FileNotFoundError(f"Metadata path not found: {metadata_path}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    torch_dtype = torch.bfloat16 if device.type == "cuda" else torch.float32

    print(f"Loading tokenizer from: {adapter_path}")
    tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    print(f"Loading base model from: {base_model_path}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch_dtype,
        device_map="auto" if device.type == "cuda" else None,
        trust_remote_code=True,
    )

    print(f"Loading LoRA adapter from: {adapter_path}")
    model = PeftModel.from_pretrained(base_model, adapter_path)
    model = model.to(device)
    model.eval()

    print(f"Reading dataset: {dataset_path}")
    with dataset_path.open("r", encoding="utf-8") as f:
        raw_data = json.load(f)

    if not isinstance(raw_data, list) or len(raw_data) == 0:
        raise ValueError("Dataset must be a non-empty list of samples.")
    metadata_records = load_metadata(metadata_path)
    if metadata_records is not None and len(metadata_records) != len(raw_data):
        raise ValueError(
            "Metadata size does not match dataset size: "
            f"{len(metadata_records)} vs {len(raw_data)}."
        )

    total_samples = len(raw_data)
    num_samples = total_samples if args.num_samples <= 0 else min(args.num_samples, total_samples)
    print(f"Evaluating {num_samples} sample(s) out of {total_samples}.\n")

    if output_dir is not None:
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"Submission files will be written to: {output_dir}")
    if results_jsonl is not None:
        results_jsonl.parent.mkdir(parents=True, exist_ok=True)
        print(f"Aggregated predictions will be saved to: {results_jsonl}")

    preview_limit = max(0, args.preview)
    results: List[dict] = []

    generation_config = {
        "max_new_tokens": args.max_new_tokens,
        "temperature": args.temperature,
        "top_p": args.top_p,
        "do_sample": args.temperature > 0,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
    }

    for idx in range(num_samples):
        sample = raw_data[idx]
        meta = metadata_records[idx] if metadata_records is not None else {}
        instance_id = sanitize_instance_id(meta.get("instance_id", f"sample_{idx:04d}"))
        reference_sql = meta.get("output") or sample.get("output", "")

        prompt = build_prompt(sample)
        messages = [{"role": "user", "content": prompt}]
        model_inputs = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt",
        ).to(device)

        attention_mask = (model_inputs != tokenizer.pad_token_id).long()

        with torch.no_grad():
            output_ids = model.generate(
                input_ids=model_inputs,
                attention_mask=attention_mask,
                **generation_config,
            )

        generated_tokens = output_ids[0, model_inputs.shape[1]:]
        response_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

        result_record = {
            "instance_id": instance_id,
            "prompt": prompt,
            "prediction": response_text,
            "reference": reference_sql,
        }
        results.append(result_record)

        if output_dir is not None:
            write_sql_file(output_dir, instance_id, response_text)

        if idx < preview_limit:
            print(f"========== Sample {idx + 1} ({instance_id}) ==========")
            print("Prompt:")
            print(prompt)
            print("\nModel output:")
            print(response_text if response_text else "<empty response>")
            if reference_sql:
                print("\nReference SQL:")
                print(reference_sql)
            print("=================================\n")

    if results_jsonl is not None:
        with results_jsonl.open("w", encoding="utf-8") as outfile:
            for record in results:
                outfile.write(json.dumps(record, ensure_ascii=False) + "\n")

    print(f"Finished generating {len(results)} prediction(s).")


if __name__ == "__main__":
    main()
