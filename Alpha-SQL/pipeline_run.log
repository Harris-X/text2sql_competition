nohup: ignoring input
[PIPELINE] 日志文件: logs/pipeline_20251126_022003.log
[PIPELINE] 检测到已有 tasks.pkl，跳过预处理。
[PIPELINE] 运行 MCTS 搜索 (配置: config/mysql_exp.yaml)
Ignore done task ids: []
Saving config to results/mysql_exp/config.json
There are 1 tasks to solve
Solving tasks (sequential):   0%|          | 0/1 [00:00<?, ?it/s]2025-11-26 02:20:04,706 | INFO | Logger initialized. File: logs/run_20251126_022004.log
2025-11-26 02:20:04,706 | INFO | Solve start task_id=0 max_rollout=24
2025-11-26 02:20:04,715 | INFO | Rollout task=0 step=1/24
2025-11-26 02:20:08,656 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=3.91s n=3
2025-11-26 02:20:12,842 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=4.06s n=3
2025-11-26 02:20:25,170 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=12.00s n=3
2025-11-26 02:20:36,677 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=10.81s n=3
2025-11-26 02:21:25,355 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=47.18s n=5
2025-11-26 02:22:02,736 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=36.29s n=4
2025-11-26 02:22:21,685 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.18s n=3
2025-11-26 02:23:32,390 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=70.32s n=3
2025-11-26 02:24:14,388 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=41.61s n=3
2025-11-26 02:24:33,423 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.40s n=2
2025-11-26 02:24:53,264 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=19.45s n=3
2025-11-26 02:25:02,588 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=7.83s n=3
2025-11-26 02:26:22,120 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=77.12s n=5
2025-11-26 02:27:27,531 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=64.79s n=5
2025-11-26 02:28:24,927 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=56.20s n=3
2025-11-26 02:28:57,798 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=32.23s n=3
2025-11-26 02:29:28,431 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=29.91s n=3
2025-11-26 02:29:40,470 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=11.48s n=3
2025-11-26 02:31:04,762 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=81.48s n=5
2025-11-26 02:31:38,589 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=33.00s n=4
2025-11-26 02:32:39,139 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=59.92s n=4
2025-11-26 02:33:22,947 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=43.01s n=3
2025-11-26 02:34:25,962 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=62.34s n=3
2025-11-26 02:34:52,894 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=25.65s n=5
2025-11-26 02:35:02,766 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=8.57s n=1
2025-11-26 02:35:22,247 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=17.59s n=5
2025-11-26 02:35:23,450 | INFO | Simulation reached terminal depth=6
2025-11-26 02:35:23,450 | INFO | Backpropagate start final_sql="WITH modelaunchdates AS (SELECT '广域战场' AS mode_name, '20240723' AS launch_date UNION ALL SELECT '消灭战', '20230804' UNION ALL SELECT '幻想混战', '20241115' UNION ALL SELECT '荒野传说', '20240903' UNION ALL SEL
2025-11-26 02:35:23,450 | INFO | Backpropagate done reward=1.0
2025-11-26 02:35:23,450 | INFO | Rollout task=0 step=2/24
2025-11-26 02:35:36,634 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=13.16s n=3
2025-11-26 02:35:43,795 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=4.25s n=3
2025-11-26 02:36:26,902 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=38.94s n=5
2025-11-26 02:36:58,451 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=30.42s n=5
2025-11-26 02:37:33,284 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=33.92s n=4
2025-11-26 02:37:46,104 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=12.11s n=2
2025-11-26 02:37:47,901 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=1.36s n=3
2025-11-26 02:38:01,943 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=9.15s n=5
2025-11-26 02:38:23,208 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=20.41s n=5
2025-11-26 02:38:45,444 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=21.34s n=5
2025-11-26 02:39:05,720 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=19.12s n=5
2025-11-26 02:39:34,196 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=26.19s n=5
2025-11-26 02:39:56,651 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=21.53s n=5
2025-11-26 02:40:14,055 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=16.33s n=5
2025-11-26 02:40:44,947 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=30.21s n=5
2025-11-26 02:41:07,682 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.74s n=5
2025-11-26 02:41:26,735 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.48s n=5
2025-11-26 02:42:10,202 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=42.83s n=5
2025-11-26 02:42:31,048 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=20.26s n=5
2025-11-26 02:42:33,701 | INFO | Simulation reached terminal depth=6
2025-11-26 02:42:33,701 | INFO | Backpropagate start final_sql="WITH launch_dates AS (SELECT '消灭战' AS mode_name, '20230804' AS launch_date UNION ALL SELECT '幻想混战', '20241115' UNION ALL SELECT '荒野传说', '20240903' UNION ALL SELECT '策略载具', '20241010' UNION ALL SELECT
2025-11-26 02:42:33,701 | INFO | Backpropagate done reward=0
2025-11-26 02:42:33,701 | INFO | Rollout task=0 step=3/24
2025-11-26 02:42:35,493 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=1.77s n=3
2025-11-26 02:42:41,551 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=1.46s n=3
2025-11-26 02:42:55,653 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=7.54s n=5
2025-11-26 02:43:13,328 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=16.72s n=5
2025-11-26 02:43:20,725 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=6.39s n=2
2025-11-26 02:43:39,427 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.30s n=2
2025-11-26 02:43:48,828 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=8.74s n=1
2025-11-26 02:43:52,593 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=3.55s n=1
2025-11-26 02:44:03,965 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=11.16s n=1
2025-11-26 02:44:05,826 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=1.65s n=3
2025-11-26 02:45:15,324 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=61.66s n=5
2025-11-26 02:46:00,881 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=44.54s n=4
2025-11-26 02:46:59,817 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=58.09s n=2
2025-11-26 02:47:31,097 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=28.69s n=5
2025-11-26 02:47:46,371 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=14.28s n=3
2025-11-26 02:47:50,961 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=3.96s n=2
2025-11-26 02:48:03,498 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=12.13s n=1
2025-11-26 02:48:32,990 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=29.27s n=1
2025-11-26 02:48:50,640 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=13.25s n=5
2025-11-26 02:49:00,194 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=8.97s n=4
2025-11-26 02:49:08,927 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=8.69s n=3
2025-11-26 02:49:15,272 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=6.31s n=2
2025-11-26 02:49:30,892 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=15.59s n=2
2025-11-26 02:49:36,541 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=5.41s n=1
2025-11-26 02:49:41,680 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=5.11s n=1
2025-11-26 02:49:48,123 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=6.41s n=1
2025-11-26 02:49:50,167 | INFO | Simulation reached terminal depth=6
2025-11-26 02:49:50,167 | INFO | Backpropagate start final_sql="WITH first_week_players AS (SELECT vplayerid, modename, submodename, dtstatdate AS first_play_date FROM dws_jordass_mode_roundrecord_di WHERE (modename = '组队竞技' AND submodename LIKE '%消灭战模式%') OR (mo
2025-11-26 02:49:50,167 | INFO | Backpropagate done reward=1.0
2025-11-26 02:49:50,167 | INFO | Rollout task=0 step=4/24
2025-11-26 02:49:56,241 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=6.05s n=3
2025-11-26 02:50:15,381 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=12.38s n=3
2025-11-26 02:51:52,344 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=87.35s n=5
2025-11-26 02:53:03,611 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=70.42s n=5
2025-11-26 02:53:35,544 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=30.95s n=3
2025-11-26 02:54:23,660 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=47.51s n=3
2025-11-26 02:55:08,063 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=44.00s n=3
2025-11-26 02:55:28,786 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=20.09s n=2
2025-11-26 02:55:36,950 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=7.79s n=3
2025-11-26 02:57:09,350 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=82.48s n=5
2025-11-26 02:57:53,746 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=43.59s n=4
2025-11-26 02:58:15,757 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=21.20s n=1
2025-11-26 02:58:40,062 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=24.09s n=1
2025-11-26 02:58:50,900 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=10.63s n=1
2025-11-26 02:59:19,473 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=28.38s n=1
2025-11-26 03:00:16,420 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=56.74s n=1
2025-11-26 03:01:07,271 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=50.83s n=1
2025-11-26 03:01:25,103 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=17.61s n=1
2025-11-26 03:01:56,383 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=31.06s n=1
2025-11-26 03:02:07,969 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=11.56s n=1
2025-11-26 03:02:39,942 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=29.34s n=5
2025-11-26 03:03:06,491 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=25.64s n=5
2025-11-26 03:03:27,747 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=20.27s n=5
2025-11-26 03:03:50,158 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=21.48s n=5
2025-11-26 03:04:14,212 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.03s n=5
2025-11-26 03:04:31,620 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=16.44s n=5
2025-11-26 03:04:47,028 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=14.47s n=5
2025-11-26 03:04:59,295 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=11.32s n=4
2025-11-26 03:05:00,034 | INFO | Simulation reached terminal depth=5
2025-11-26 03:05:00,034 | INFO | Backpropagate start final_sql="WITH mode_launch_dates AS (SELECT '广域战场' AS gamemode, '20240723' AS launch_date UNION ALL SELECT '消灭战', '20230804' UNION ALL SELECT '幻想混战', '20241115' UNION ALL SELECT '荒野传说', '20240903' UNION ALL SE
2025-11-26 03:05:00,035 | INFO | Backpropagate done reward=0
2025-11-26 03:05:00,035 | INFO | Rollout task=0 step=5/24
2025-11-26 03:05:08,552 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=8.50s n=3
2025-11-26 03:05:21,290 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=4.70s n=3
2025-11-26 03:05:49,996 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=20.19s n=3
2025-11-26 03:06:44,477 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=42.41s n=5
2025-11-26 03:07:22,804 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=37.36s n=4
2025-11-26 03:08:04,284 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=40.93s n=4
2025-11-26 03:08:48,293 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=43.45s n=4
2025-11-26 03:09:16,708 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=27.65s n=3
2025-11-26 03:09:20,355 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=3.19s n=3
2025-11-26 03:09:41,057 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=10.99s n=3
2025-11-26 03:10:23,281 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=29.74s n=5
2025-11-26 03:11:05,654 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=40.81s n=5
2025-11-26 03:11:31,620 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=24.98s n=5
2025-11-26 03:12:27,352 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=54.81s n=5
2025-11-26 03:12:29,355 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=1.25s n=3
2025-11-26 03:13:06,954 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=24.32s n=5
2025-11-26 03:13:28,359 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=20.47s n=5
2025-11-26 03:13:48,757 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=19.38s n=5
2025-11-26 03:14:14,710 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=24.98s n=5
2025-11-26 03:14:33,148 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=13.55s n=5
2025-11-26 03:14:53,971 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=19.88s n=5
2025-11-26 03:15:13,214 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.49s n=5
2025-11-26 03:15:26,033 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=11.83s n=5
2025-11-26 03:15:45,800 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=11.84s n=5
2025-11-26 03:15:55,094 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=8.29s n=3
2025-11-26 03:16:05,845 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=10.33s n=3
2025-11-26 03:16:12,306 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=5.87s n=2
2025-11-26 03:16:19,999 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=7.29s n=2
2025-11-26 03:16:22,051 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=1.64s n=1
2025-11-26 03:16:27,474 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=5.40s n=1
2025-11-26 03:16:27,671 | INFO | Simulation reached terminal depth=6
2025-11-26 03:16:27,671 | INFO | Backpropagate start final_sql="WITH firstlaunchdates AS (SELECT `modename`, `startdate` FROM `dim_jordass_imode_leyuan_nf`), firsttimeparticipants AS (SELECT fl.`modename`, fl.`startdate`, p.`vplayerid`, MIN(p.`dteventtime`) AS fi
2025-11-26 03:16:27,671 | INFO | Backpropagate done reward=0
2025-11-26 03:16:27,671 | INFO | Rollout task=0 step=6/24
2025-11-26 03:16:28,786 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=1.09s n=3
2025-11-26 03:16:40,703 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=0.87s n=3
2025-11-26 03:17:07,492 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=11.72s n=5
2025-11-26 03:17:27,370 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.79s n=4
2025-11-26 03:17:42,691 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=14.51s n=3
2025-11-26 03:17:46,818 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=3.50s n=1
2025-11-26 03:17:49,087 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=2.04s n=1
2025-11-26 03:17:52,241 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=2.96s n=1
2025-11-26 03:17:55,994 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=3.56s n=1
2025-11-26 03:17:59,083 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=2.85s n=1
2025-11-26 03:18:02,807 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=3.52s n=1
2025-11-26 03:18:06,777 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=3.77s n=1
2025-11-26 03:18:07,842 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=0.84s n=3
2025-11-26 03:18:45,882 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=22.32s n=5
2025-11-26 03:19:20,407 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=33.45s n=4
2025-11-26 03:19:36,471 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=14.71s n=2
2025-11-26 03:19:48,675 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=7.52s n=5
2025-11-26 03:19:56,131 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=6.40s n=1
2025-11-26 03:20:16,183 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=11.15s n=5
2025-11-26 03:20:16,411 | INFO | Simulation reached terminal depth=5
2025-11-26 03:20:16,411 | INFO | Backpropagate start final_sql="WITH firstplaydates AS (SELECT vplayerid, modename, submodename, MIN(dtstatdate) AS first_play_date FROM dws_jordass_mode_roundrecord_di WHERE (modename = '休闲模式' AND submodename = '广域战场模式') OR (moden
2025-11-26 03:20:16,411 | INFO | Backpropagate done reward=0.4
2025-11-26 03:20:16,411 | INFO | Rollout task=0 step=7/24
2025-11-26 03:20:19,479 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=3.04s n=3
2025-11-26 03:20:45,115 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=12.22s n=3
2025-11-26 03:21:42,879 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=39.10s n=5
2025-11-26 03:22:23,831 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=39.99s n=4
2025-11-26 03:22:51,603 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=26.95s n=2
2025-11-26 03:23:04,684 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=12.66s n=1
2025-11-26 03:23:22,292 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=17.38s n=1
2025-11-26 03:23:32,308 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=9.78s n=1
2025-11-26 03:23:54,995 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=22.46s n=1
2025-11-26 03:24:07,012 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=11.78s n=3
2025-11-26 03:24:59,285 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=33.97s n=5
2025-11-26 03:25:18,964 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.85s n=2
2025-11-26 03:25:45,448 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=20.92s n=5
2025-11-26 03:25:53,221 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=6.69s n=1
2025-11-26 03:26:23,568 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=19.98s n=5
2025-11-26 03:26:46,526 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=21.88s n=5
2025-11-26 03:27:01,738 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=14.09s n=4
2025-11-26 03:27:13,992 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=11.37s n=2
2025-11-26 03:27:14,244 | INFO | Simulation reached terminal depth=5
2025-11-26 03:27:14,244 | INFO | Backpropagate start final_sql="WITH firstplaydates AS (SELECT CASE WHEN `submodename` = '广域战场模式' THEN '广域战场' WHEN `modename` = '组队竞技' AND `submodename` LIKE '%消灭战模式%' THEN '消灭战' WHEN `modename` = '创意创作间' AND `submodename` = '幻想混战'
2025-11-26 03:27:14,244 | INFO | Backpropagate done reward=0.4
2025-11-26 03:27:14,244 | INFO | Rollout task=0 step=8/24
2025-11-26 03:27:15,309 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=1.04s n=3
2025-11-26 03:27:31,159 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=0.66s n=3
2025-11-26 03:28:04,642 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=13.53s n=5
2025-11-26 03:28:21,395 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=15.75s n=4
2025-11-26 03:28:45,637 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=23.42s n=3
2025-11-26 03:28:54,690 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=8.46s n=2
2025-11-26 03:28:58,113 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=3.04s n=2
2025-11-26 03:28:59,138 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=0.79s n=3
2025-11-26 03:29:36,868 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=16.92s n=5
2025-11-26 03:29:49,968 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=12.08s n=4
2025-11-26 03:30:10,270 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=19.45s n=4
2025-11-26 03:30:36,678 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=25.55s n=2
2025-11-26 03:30:53,984 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=16.87s n=2
2025-11-26 03:31:24,257 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.15s n=5
2025-11-26 03:31:39,998 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=14.72s n=5
2025-11-26 03:31:59,024 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.23s n=5
2025-11-26 03:32:17,935 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=18.05s n=5
2025-11-26 03:32:24,607 | INFO | Simulation reached terminal depth=5
2025-11-26 03:32:24,608 | INFO | Backpropagate start final_sql="WITH mode_launch_dates AS (SELECT '广域战场' AS mode_name, '20240723' AS launch_date UNION ALL SELECT '消灭战', '20230804' UNION ALL SELECT '幻想混战', '20241115' UNION ALL SELECT '荒野传说', '20240903' UNION ALL S
2025-11-26 03:32:24,608 | INFO | Backpropagate done reward=0
2025-11-26 03:32:24,608 | INFO | Rollout task=0 step=9/24
2025-11-26 03:32:33,318 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=8.69s n=3
2025-11-26 03:32:55,876 | INFO | LLM SDK success model=Qwen3-Coder-30B-A3B-Instruct latency=6.01s n=3
2025-11-26 03:33:19,575 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:33:29,621 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:33:39,665 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:33:49,710 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:33:59,755 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:33:59,817 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:34:09,864 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:34:19,909 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:34:29,953 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:34:39,998 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:34:40,062 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:34:50,108 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:35:00,154 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:35:10,200 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:35:20,246 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:35:20,310 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:35:30,355 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:35:40,399 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:35:50,447 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:36:00,493 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:36:00,558 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:36:10,604 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:36:20,648 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:36:30,693 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:36:40,744 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:36:40,806 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:36:50,856 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:37:00,901 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:37:10,946 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:37:20,991 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:37:21,055 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:37:31,099 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:37:41,145 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:37:51,191 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:38:01,236 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:38:01,299 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:38:11,344 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:38:21,388 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:38:31,435 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:38:41,483 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:38:41,544 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:38:51,590 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:39:01,635 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:39:11,680 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:39:21,724 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:39:21,787 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:39:31,831 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:39:41,878 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:39:51,923 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:40:01,967 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:40:02,033 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:40:12,077 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:40:22,123 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:40:32,167 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:40:42,212 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:40:42,274 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:40:52,319 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:41:02,367 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:41:12,413 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:41:22,459 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:41:22,521 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:41:32,566 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:41:42,613 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:41:52,660 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:42:02,705 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:42:02,768 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:42:12,813 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:42:22,859 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:42:32,905 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:42:42,950 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:42:43,013 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:42:53,057 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:43:03,103 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:43:13,148 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:43:23,208 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:43:23,285 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:43:33,345 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:43:43,390 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:43:53,435 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:44:03,480 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:44:03,542 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:44:13,587 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:44:23,631 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:44:33,676 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:44:43,721 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:44:43,782 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:44:53,826 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:45:03,873 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:45:13,919 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:45:23,964 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:45:24,026 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:45:34,072 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:45:44,120 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:45:54,188 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:46:04,249 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:46:04,325 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:46:14,371 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:46:24,416 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:46:34,462 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:46:44,507 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:46:44,570 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:46:54,616 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:47:04,663 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:47:14,712 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:47:24,756 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:47:24,818 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:47:34,865 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:47:44,911 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:47:54,956 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:48:05,000 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:48:05,063 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:48:15,109 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:48:25,157 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:48:35,203 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:48:45,249 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:48:45,315 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:48:55,362 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:49:05,407 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:49:15,461 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:49:25,505 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:49:25,566 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:49:35,613 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:49:45,658 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:49:55,702 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:50:05,747 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:50:05,811 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:50:15,856 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:50:25,902 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:50:35,948 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:50:45,992 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:50:46,065 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:50:56,111 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:51:06,158 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:51:16,203 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:51:26,249 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:51:26,320 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:51:36,366 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:51:46,414 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:51:56,458 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:52:06,503 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:52:06,586 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:52:16,633 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:52:26,678 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:52:36,723 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:52:46,769 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:52:46,847 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:52:56,897 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:53:06,942 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:53:16,990 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:53:27,034 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:53:27,100 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:53:37,145 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:53:47,212 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:53:57,257 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:54:07,305 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:54:07,375 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:54:17,422 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:54:27,469 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:54:37,513 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:54:47,558 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:54:47,619 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:54:57,664 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:55:07,715 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:55:17,760 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:55:27,806 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:55:27,873 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:55:37,918 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:55:47,964 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:55:58,009 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:56:08,055 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:56:08,118 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:56:18,162 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:56:28,207 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:56:38,252 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:56:48,296 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:56:48,369 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:56:58,415 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:57:08,460 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:57:18,504 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:57:28,549 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:57:28,610 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:57:38,655 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:57:48,700 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:57:58,747 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:58:08,791 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:58:08,853 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:58:18,897 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:58:28,941 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:58:38,988 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:58:49,034 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:58:49,098 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:58:59,148 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:59:09,195 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:59:19,241 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:59:29,286 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:59:29,346 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:59:39,392 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:59:49,445 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 03:59:59,490 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:00:09,537 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:00:09,603 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:00:19,647 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:00:29,693 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:00:39,739 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:00:49,784 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:00:49,845 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:00:59,890 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:01:09,936 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:01:19,983 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:01:30,028 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:01:30,089 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:01:40,134 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:01:50,179 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:02:00,227 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:02:10,273 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:02:10,335 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:02:20,388 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:02:30,432 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:02:40,477 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:02:50,523 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:02:50,587 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:03:00,632 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:03:10,679 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:03:20,725 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:03:30,769 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:03:30,833 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:03:40,877 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:03:50,921 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:04:00,967 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:04:11,011 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:04:11,075 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:04:21,120 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:04:31,163 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:04:41,209 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:04:51,255 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:04:51,315 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:05:01,360 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:05:11,419 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:05:21,467 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:05:31,512 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:05:31,575 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:05:41,621 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:05:51,667 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:06:01,713 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:06:11,760 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:06:11,820 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:06:21,865 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:06:31,909 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:06:41,954 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:06:51,999 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:06:52,062 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:07:02,106 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:07:12,150 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:07:22,194 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:07:32,238 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:07:32,300 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:07:42,346 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:07:52,392 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:08:02,437 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:08:12,483 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:08:12,545 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:08:22,590 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:08:32,634 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:08:42,681 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:08:52,727 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:08:52,788 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:09:02,833 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:09:12,878 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:09:22,923 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:09:32,968 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:09:33,029 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:09:43,074 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:09:53,119 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:10:03,164 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:10:13,209 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:10:13,273 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:10:23,317 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:10:33,362 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:10:43,408 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:10:53,453 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:10:53,515 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:11:03,560 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:11:13,605 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:11:23,652 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:11:33,696 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:11:33,760 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:11:43,804 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:11:53,848 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:12:03,896 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:12:13,941 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:12:14,003 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:12:24,048 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:12:34,093 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:12:44,137 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:12:54,187 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:12:54,249 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:13:04,294 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:13:14,342 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:13:24,401 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:13:34,446 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:13:34,512 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:13:44,557 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:13:54,602 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:14:04,647 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:14:14,693 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:14:14,756 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:14:24,817 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:14:34,863 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:14:44,908 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:14:54,953 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:14:55,015 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:15:05,061 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:15:15,107 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:15:25,154 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:15:35,199 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:15:35,261 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:15:45,305 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:15:55,351 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:16:05,396 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:16:15,441 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:16:15,502 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:16:25,546 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:16:35,591 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:16:45,651 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:16:55,696 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:16:55,767 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:17:05,814 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:17:15,861 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:17:25,906 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:17:35,950 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:17:36,011 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:17:46,056 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:17:56,100 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:18:06,145 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:18:16,191 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:18:16,252 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:18:26,298 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:18:36,344 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:18:46,391 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:18:56,443 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:18:56,507 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:19:06,553 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:19:16,599 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:19:26,644 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:19:36,690 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:19:36,751 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:19:46,797 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:19:56,842 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:20:06,887 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:20:16,932 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:20:16,995 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:20:27,041 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:20:37,086 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:20:47,131 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:20:57,176 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:20:57,240 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:21:07,285 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:21:17,331 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:21:27,375 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:21:37,420 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:21:37,480 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:21:47,527 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:21:57,572 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:22:07,621 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:22:17,666 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:22:17,729 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:22:27,774 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:22:37,820 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:22:47,885 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:22:57,929 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:22:57,992 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:23:08,037 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:23:18,081 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:23:28,125 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:23:38,170 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:23:38,235 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:23:48,279 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:23:58,323 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:24:08,368 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:24:18,415 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:24:18,479 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:24:28,523 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:24:38,568 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:24:48,614 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:24:58,657 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql revision response: 'NoneType' object has no attribute 'group'
Error parsing sql revision response: 'NoneType' object has no attribute 'group'
Error parsing sql revision response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql revision response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql revision response: 'NoneType' object has no attribute 'group'
Error parsing sql revision response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
2025-11-26 04:24:58,720 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:25:08,765 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:25:18,810 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:25:28,857 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:25:38,902 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:25:38,965 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:25:49,009 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:25:59,054 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:26:09,114 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:26:19,173 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:26:19,246 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:26:29,305 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:26:39,364 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:26:49,423 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:26:59,467 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:26:59,530 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:27:09,577 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:27:19,621 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:27:29,664 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:27:39,713 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:27:39,779 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:27:49,835 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:27:59,879 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:28:09,931 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:28:19,977 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:28:20,039 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:28:30,097 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:28:40,142 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:28:50,192 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:29:00,237 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:29:00,299 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:29:10,342 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:29:20,386 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:29:30,431 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:29:40,476 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:29:40,537 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:29:50,581 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:30:00,626 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:30:10,670 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:30:20,715 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:30:20,778 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:30:30,822 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:30:40,866 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:30:50,911 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:31:00,958 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:31:01,026 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:31:11,069 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:31:21,113 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:31:31,158 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:31:41,205 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:31:41,267 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:31:51,312 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:32:01,356 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:32:11,403 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:32:21,447 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:32:21,509 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:32:31,553 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:32:41,597 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:32:51,642 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:33:01,686 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:33:01,748 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:33:11,792 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:33:21,837 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:33:31,882 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:33:41,926 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:33:41,990 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:33:52,035 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:34:02,099 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:34:12,152 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:34:22,197 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:34:22,259 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:34:32,303 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:34:42,348 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:34:52,394 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:35:02,438 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:35:02,499 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:35:12,544 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:35:22,589 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:35:32,636 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:35:42,699 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:35:42,779 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:35:52,824 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:36:02,869 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:36:12,914 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:36:22,959 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:36:23,020 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:36:33,064 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:36:43,110 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:36:53,156 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:37:03,201 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:37:03,273 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:37:13,321 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:37:23,367 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:37:33,414 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:37:43,460 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:37:43,522 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:37:53,569 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:38:03,616 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:38:13,660 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:38:23,705 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:38:23,768 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:38:33,815 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:38:43,860 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:38:53,906 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:39:03,952 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:39:04,015 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:39:14,060 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:39:24,105 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:39:34,149 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:39:44,195 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:39:44,256 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:39:54,332 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:40:04,377 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:40:14,422 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:40:24,467 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:40:24,531 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:40:34,575 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:40:44,620 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:40:54,665 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:41:04,711 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:41:04,773 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:41:14,818 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:41:24,863 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:41:34,908 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:41:44,952 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:41:45,013 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:41:55,058 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:42:05,103 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:42:15,149 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:42:25,196 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:42:25,258 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:42:35,303 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:42:45,347 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:42:55,391 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:43:05,439 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:43:05,513 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:43:15,558 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:43:25,605 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:43:35,650 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:43:45,696 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:43:45,760 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:43:55,806 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:44:05,852 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:44:15,897 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:44:25,943 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:44:26,007 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:44:36,053 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:44:46,099 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:44:56,144 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:45:06,204 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:45:06,283 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:45:16,328 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:45:26,389 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:45:36,436 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:45:46,480 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:45:46,542 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:45:56,601 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:46:06,647 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:46:16,691 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:46:26,740 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:46:26,801 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:46:36,852 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:46:46,900 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:46:56,948 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:47:06,994 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:47:07,060 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:47:17,106 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:47:27,153 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:47:37,219 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:47:47,277 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:47:47,340 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:47:57,386 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:48:07,431 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:48:17,495 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:48:27,559 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:48:27,638 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:48:37,683 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:48:47,727 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:48:57,775 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:49:07,819 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:49:07,881 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:49:17,925 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:49:27,969 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:49:38,015 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:49:48,062 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:49:48,126 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:49:58,175 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:50:08,221 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:50:18,266 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:50:28,312 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:50:28,376 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:50:38,423 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:50:48,468 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:50:58,513 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:51:08,558 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:51:08,622 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:51:18,672 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:51:28,719 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:51:38,764 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:51:48,809 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:51:48,870 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:51:58,914 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:52:08,963 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:52:19,009 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:52:29,054 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:52:29,118 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:52:39,162 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:52:49,208 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:52:59,253 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:53:09,299 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:53:09,361 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:53:19,407 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:53:29,453 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:53:39,498 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:53:49,542 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:53:49,607 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:53:59,652 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:54:09,699 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:54:19,743 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:54:29,800 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:54:29,870 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:54:39,924 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:54:49,969 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:55:00,013 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:55:10,061 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:55:10,129 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:55:20,174 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:55:30,220 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:55:40,264 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:55:50,310 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:55:50,371 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:56:00,416 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:56:10,463 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:56:20,508 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:56:30,553 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:56:30,616 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:56:40,661 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:56:50,708 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:57:00,752 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:57:10,808 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:57:10,871 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:57:20,915 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:57:30,961 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:57:41,007 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:57:51,051 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:57:51,114 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:58:01,158 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:58:11,202 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:58:21,247 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:58:31,292 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:58:31,353 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:58:41,401 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:58:51,448 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:59:01,493 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:59:11,538 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:59:11,601 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:59:21,647 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:59:31,693 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:59:41,738 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:59:51,783 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 04:59:51,844 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:00:01,890 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:00:11,935 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:00:21,980 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:00:32,027 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:00:32,090 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:00:42,137 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:00:52,183 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:01:02,227 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:01:12,272 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:01:12,335 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:01:22,380 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:01:32,425 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:01:42,471 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:01:52,516 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:01:52,580 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:02:02,632 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:02:12,679 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:02:22,725 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:02:32,770 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:02:32,832 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:02:42,876 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:02:52,921 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:03:02,965 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:03:13,011 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:03:13,073 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:03:23,118 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:03:33,164 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:03:43,212 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:03:53,258 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:03:53,320 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:04:03,365 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:04:13,410 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:04:23,456 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:04:33,502 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:04:33,569 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:04:43,615 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:04:53,661 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:05:03,706 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:05:13,751 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:05:13,814 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:05:23,862 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:05:33,907 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:05:43,952 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:05:54,000 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:05:54,065 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:06:04,110 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:06:14,155 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:06:24,200 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:06:34,245 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:06:34,308 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:06:44,356 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:06:54,401 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:07:04,446 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:07:14,493 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:07:14,554 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:07:24,599 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:07:34,644 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:07:44,690 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:07:54,745 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:07:54,819 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:08:04,865 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:08:14,909 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:08:24,954 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:08:34,998 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:08:35,061 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:08:45,108 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:08:55,154 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:09:05,198 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:09:15,243 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:09:15,306 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:09:25,352 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:09:35,396 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:09:45,442 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:09:55,488 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:09:55,553 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:10:05,598 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:10:15,643 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:10:25,691 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:10:35,739 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:10:35,800 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:10:45,845 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:10:55,890 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:11:05,934 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:11:15,979 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:11:16,040 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:11:26,086 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:11:36,131 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:11:46,175 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:11:56,221 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:11:56,282 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:12:06,330 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:12:16,375 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:12:26,421 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:12:36,466 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:12:36,532 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:12:46,578 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:12:56,623 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:13:06,668 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:13:16,714 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:13:16,777 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:13:26,822 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:13:36,866 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:13:46,911 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:13:56,959 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:13:57,023 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:14:07,071 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:14:17,117 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:14:27,162 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:14:37,208 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:14:37,273 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:14:47,318 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:14:57,366 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:15:07,410 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:15:17,455 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:15:17,519 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:15:27,567 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:15:37,612 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:15:47,657 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:15:57,710 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:15:57,778 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:16:07,823 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:16:17,869 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:16:27,914 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:16:37,964 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:16:38,035 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:16:48,079 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:16:58,124 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:17:08,183 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:17:18,229 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:17:18,301 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:17:28,347 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:17:38,392 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:17:48,438 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:17:58,481 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:17:58,544 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:18:08,589 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:18:18,634 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:18:28,679 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:18:38,724 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:18:38,789 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:18:48,834 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:18:58,878 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:19:08,925 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:19:18,969 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:19:19,032 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:19:29,076 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:19:39,124 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:19:49,169 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:19:59,214 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:19:59,276 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:20:09,322 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:20:19,369 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:20:29,419 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:20:39,464 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:20:39,526 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:20:49,571 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:20:59,616 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:21:09,663 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:21:19,708 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:21:19,770 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:21:29,814 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:21:39,863 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:21:49,907 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:21:59,952 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:22:00,018 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:22:10,072 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:22:20,118 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:22:30,165 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:22:40,211 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:22:40,275 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:22:50,321 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:23:00,366 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:23:10,413 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:23:20,458 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:23:20,521 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:23:30,586 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:23:40,630 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:23:50,677 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:24:00,721 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:24:00,783 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:24:10,829 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:24:20,873 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:24:30,919 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:24:40,965 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:24:41,026 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:24:51,072 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:25:01,117 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:25:11,163 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:25:21,208 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:25:21,271 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:25:31,319 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:25:41,364 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:25:51,410 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:26:01,455 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:26:01,519 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:26:11,564 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:26:21,609 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:26:31,656 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:26:41,701 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:26:41,762 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:26:51,807 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:27:01,853 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:27:11,900 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:27:21,944 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:27:22,006 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:27:32,051 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:27:42,096 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:27:52,143 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:28:02,188 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:28:02,249 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:28:12,293 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:28:22,338 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:28:32,383 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:28:42,429 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:28:42,496 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:28:52,541 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:29:02,586 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:29:12,634 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:29:22,679 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:29:22,744 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:29:32,788 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:29:42,834 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:29:52,879 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:30:02,923 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:30:02,985 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:30:13,031 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:30:23,076 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:30:33,123 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:30:43,169 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:30:43,231 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:30:53,276 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:31:03,321 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:31:13,366 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:31:23,411 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
2025-11-26 05:31:23,473 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:31:33,518 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:31:43,563 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:31:53,607 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:32:03,651 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:32:03,720 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:32:13,764 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:32:23,810 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:32:33,854 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:32:43,898 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:32:43,962 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:32:54,006 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:33:04,051 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:33:14,097 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:33:24,141 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:33:24,204 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:33:34,251 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:33:44,295 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:33:54,344 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:34:04,388 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:34:04,452 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:34:14,498 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:34:24,544 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:34:34,588 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:34:44,633 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:34:44,694 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:34:54,739 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:35:04,784 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:35:14,829 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:35:24,875 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:35:24,941 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:35:34,989 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:35:45,037 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:35:55,086 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:36:05,131 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:36:05,192 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:36:15,237 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:36:25,281 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:36:35,326 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:36:45,371 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:36:45,434 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:36:55,487 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:37:05,545 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:37:15,591 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:37:25,636 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:37:25,698 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:37:35,744 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:37:45,789 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:37:55,835 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:38:05,880 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:38:05,943 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:38:15,988 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:38:26,032 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:38:36,079 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:38:46,124 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:38:46,190 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:38:56,235 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:39:06,280 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:39:16,325 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:39:26,372 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:39:26,439 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:39:36,484 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:39:46,533 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:39:56,581 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:40:06,626 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:40:06,688 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:40:16,735 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:40:26,780 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:40:36,827 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:40:46,872 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:40:46,935 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:40:56,979 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:41:07,023 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:41:17,068 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:41:27,113 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:41:27,175 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:41:37,222 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:41:47,266 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:41:57,319 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:42:07,368 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:42:07,445 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:42:17,491 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:42:27,536 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:42:37,580 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:42:47,625 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:42:47,688 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:42:57,733 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:43:07,777 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:43:17,823 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:43:27,873 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:43:27,945 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:43:37,989 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:43:48,035 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:43:58,081 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:44:08,127 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:44:08,189 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:44:18,233 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:44:28,277 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:44:38,324 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:44:48,368 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:44:48,429 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:44:58,472 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:45:08,517 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:45:18,563 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:45:28,607 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:45:28,670 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:45:38,714 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:45:48,760 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:45:58,806 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:46:08,850 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:46:08,911 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:46:18,956 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:46:29,003 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:46:39,050 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:46:49,094 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:46:49,155 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:46:59,202 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:47:09,248 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:47:19,295 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:47:29,339 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:47:29,402 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:47:39,447 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:47:49,492 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:47:59,536 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:48:09,582 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:48:09,646 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:48:19,691 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:48:29,735 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:48:39,781 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:48:49,836 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:48:49,911 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:48:59,956 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:49:10,002 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:49:20,047 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:49:30,092 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:49:30,155 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:49:40,199 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:49:50,244 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:50:00,289 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:50:10,336 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:50:10,405 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:50:20,451 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:50:30,496 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:50:40,543 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:50:50,587 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:50:50,649 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:51:00,694 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:51:10,738 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:51:20,782 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:51:30,827 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:51:30,888 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:51:40,932 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:51:50,976 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:52:01,021 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:52:11,067 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:52:11,133 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:52:21,177 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:52:31,222 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:52:41,266 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:52:51,311 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:52:51,374 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:53:01,419 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:53:11,463 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:53:21,508 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:53:31,553 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:53:31,614 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:53:41,659 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:53:51,705 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:54:01,752 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:54:11,799 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:54:11,863 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:54:21,908 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:54:31,953 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:54:41,998 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:54:52,042 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:54:52,104 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:55:02,162 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:55:12,221 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:55:22,266 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:55:32,314 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:55:32,379 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:55:42,425 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:55:52,470 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:56:02,515 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:56:12,559 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:56:12,622 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:56:22,666 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:56:32,711 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:56:42,763 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:56:52,809 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:56:52,871 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:57:02,916 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:57:12,961 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:57:23,016 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:57:33,062 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:57:33,126 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:57:43,172 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:57:53,216 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:58:03,263 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:58:13,308 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:58:13,371 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:58:23,417 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:58:33,462 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:58:43,508 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:58:53,553 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:58:53,620 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:59:03,665 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:59:13,710 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:59:23,757 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:59:33,803 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:59:33,866 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:59:43,912 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 05:59:53,957 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:00:04,002 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:00:14,047 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:00:14,110 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:00:24,155 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:00:34,200 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:00:44,248 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:00:54,293 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:00:54,355 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:01:04,401 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:01:14,446 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:01:24,491 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:01:34,536 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:01:34,599 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:01:44,644 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:01:54,689 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:02:04,736 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:02:14,780 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:02:14,844 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:02:24,889 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:02:34,942 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:02:44,986 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:02:55,031 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:02:55,096 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:03:05,140 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:03:15,185 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:03:25,231 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:03:35,277 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:03:35,343 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:03:45,387 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:03:55,433 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:04:05,496 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:04:15,549 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:04:15,613 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:04:25,659 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:04:35,704 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:04:45,750 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:04:55,796 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:04:55,862 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:05:05,907 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:05:15,954 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:05:25,999 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:05:36,045 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:05:36,113 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:05:46,159 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:05:56,205 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:06:06,250 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:06:16,296 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:06:16,359 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:06:26,403 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:06:36,450 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:06:46,495 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:06:56,541 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:06:56,602 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:07:06,647 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:07:16,693 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:07:26,742 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:07:36,787 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:07:36,849 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:07:46,895 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:07:56,940 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:08:06,987 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:08:17,033 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:08:17,094 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:08:27,138 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:08:37,185 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:08:47,231 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:08:57,290 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:08:57,368 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:09:07,425 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:09:17,470 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:09:27,518 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:09:37,564 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:09:37,630 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:09:47,674 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:09:57,719 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:10:07,764 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:10:17,809 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:10:17,872 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:10:27,918 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:10:37,963 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:10:48,012 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:10:58,057 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:10:58,122 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:11:08,168 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:11:18,214 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:11:28,261 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:11:38,306 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:11:38,371 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:11:48,435 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:11:58,483 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:12:08,528 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:12:18,572 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:12:18,635 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:12:28,680 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:12:38,725 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:12:48,771 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:12:58,817 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:12:58,885 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:13:08,930 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:13:18,975 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:13:29,021 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:13:39,066 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:13:39,126 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:13:49,173 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:13:59,219 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:14:09,269 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:14:19,315 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:14:19,381 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:14:29,428 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:14:39,474 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:14:49,519 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:14:59,563 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:14:59,625 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:15:09,673 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:15:19,718 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:15:29,764 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:15:39,809 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:15:39,872 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:15:49,917 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:15:59,963 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:16:10,009 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:16:20,053 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:16:20,115 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:16:30,160 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:16:40,207 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:16:50,252 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:17:00,296 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:17:00,368 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:17:10,412 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:17:20,456 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:17:30,512 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:17:40,558 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:17:40,620 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:17:50,665 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:18:00,710 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:18:10,769 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:18:20,824 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:18:20,887 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:18:30,934 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:18:40,982 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:18:51,026 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:19:01,073 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:19:01,139 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:19:11,185 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:19:21,231 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:19:31,275 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:19:41,322 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:19:41,391 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:19:51,441 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:20:01,486 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:20:11,531 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:20:21,575 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:20:21,648 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:20:31,693 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:20:41,738 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:20:51,786 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:21:01,834 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:21:01,898 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:21:11,943 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:21:21,989 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:21:32,033 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:21:42,078 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:21:42,143 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:21:52,188 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:22:02,233 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:22:12,278 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:22:22,324 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:22:22,389 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:22:32,434 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:22:42,479 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:22:52,524 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:23:02,568 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:23:02,631 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:23:12,676 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:23:22,721 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:23:32,765 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:23:42,811 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:23:42,872 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:23:52,917 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:24:02,962 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:24:13,009 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:24:23,054 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:24:23,119 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:24:33,168 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:24:43,213 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:24:53,259 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:25:03,304 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:25:03,364 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:25:13,412 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:25:23,460 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:25:33,517 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:25:43,564 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:25:43,629 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:25:53,689 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:26:03,733 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:26:13,778 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:26:23,825 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:26:23,888 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:26:33,933 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:26:43,977 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:26:54,022 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:27:04,067 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:27:04,130 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:27:14,175 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:27:24,221 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:27:34,269 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:27:44,314 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:27:44,381 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:27:54,428 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:28:04,474 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:28:14,521 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:28:24,566 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:28:24,628 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:28:34,673 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:28:44,718 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:28:54,762 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:29:04,806 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:29:04,872 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:29:14,918 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:29:24,962 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:29:35,007 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:29:45,051 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:29:45,114 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:29:55,159 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:30:05,203 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:30:15,247 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:30:25,292 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:30:25,354 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:30:35,399 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:30:45,444 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:30:55,491 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:31:05,536 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:31:05,600 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:31:15,645 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:31:25,690 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:31:35,736 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:31:45,782 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:31:45,843 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:31:55,888 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:32:05,934 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:32:15,978 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:32:26,024 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:32:26,092 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:32:36,136 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:32:46,181 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:32:56,226 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:33:06,272 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:33:06,341 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:33:16,387 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:33:26,433 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:33:36,478 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:33:46,524 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:33:46,587 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:33:56,632 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:34:06,677 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:34:16,725 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:34:26,773 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:34:26,840 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:34:36,885 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:34:46,930 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:34:56,974 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:35:07,019 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:35:07,080 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:35:17,125 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:35:27,169 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:35:37,217 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:35:47,265 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:35:47,333 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:35:57,377 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:36:07,423 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:36:17,469 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:36:27,513 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:36:27,577 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:36:37,623 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:36:47,669 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:36:57,714 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:37:07,759 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:37:07,823 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:37:17,867 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:37:27,910 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:37:37,955 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:37:48,000 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
Error parsing sql generation response: 'NoneType' object has no attribute 'group'
2025-11-26 06:37:48,062 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:37:58,106 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:38:08,151 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:38:18,197 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:38:28,241 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:38:28,302 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:38:38,346 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:38:48,390 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:38:58,436 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:39:08,480 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:39:08,544 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:39:18,590 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:39:28,635 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:39:38,683 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:39:48,729 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:39:48,792 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:39:58,837 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:40:08,882 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:40:18,928 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:40:28,974 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:40:29,040 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:40:39,088 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:40:49,146 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:40:59,192 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:41:09,238 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:41:09,307 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:41:19,353 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:41:29,397 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:41:39,443 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:41:49,503 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:41:49,573 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:41:59,619 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:42:09,675 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:42:19,720 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:42:29,767 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:42:29,835 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:42:39,880 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:42:49,925 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:42:59,972 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:43:10,017 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:43:10,082 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:43:20,127 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:43:30,171 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:43:40,215 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:43:50,258 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:43:50,320 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:44:00,365 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:44:10,410 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:44:20,458 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:44:30,502 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:44:30,564 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:44:40,608 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:44:50,652 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:45:00,697 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:45:10,741 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:45:10,802 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:45:20,847 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:45:30,893 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:45:40,938 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:45:50,982 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:45:51,047 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:46:01,094 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:46:11,139 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:46:21,185 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:46:31,230 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:46:31,293 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:46:41,337 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:46:51,382 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:47:01,427 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:47:11,475 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:47:11,540 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:47:21,587 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:47:31,632 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:47:41,679 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:47:51,724 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:47:51,788 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:48:01,834 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:48:11,878 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:48:21,923 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:48:31,983 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:48:32,057 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:48:42,101 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:48:52,146 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:49:02,192 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:49:12,237 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:49:12,305 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:49:22,350 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:49:32,394 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:49:42,440 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:49:52,486 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:49:52,553 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:50:02,597 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:50:12,644 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:50:22,689 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:50:32,735 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:50:32,796 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:50:42,841 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:50:52,886 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:51:02,934 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:51:12,981 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:51:13,044 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:51:23,090 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:51:33,136 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:51:43,181 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:51:53,226 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:51:53,289 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:52:03,333 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:52:13,377 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:52:23,423 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:52:33,469 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:52:33,533 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:52:43,579 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:52:53,623 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:53:03,669 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:53:13,714 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:53:13,777 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:53:23,822 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:53:33,867 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:53:43,912 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:53:53,958 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:53:54,019 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:54:04,065 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:54:14,109 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:54:24,157 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:54:34,203 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:54:34,265 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:54:44,311 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:54:54,355 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:55:04,401 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:55:14,447 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:55:14,520 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:55:24,565 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:55:34,610 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:55:44,655 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:55:54,701 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:55:54,767 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:56:04,812 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:56:14,856 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:56:24,903 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:56:34,949 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:56:35,011 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:56:45,055 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:56:55,100 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:57:05,145 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:57:15,190 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:57:15,252 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:57:25,296 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:57:35,341 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:57:45,397 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:57:55,443 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:57:55,506 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:58:05,550 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:58:15,594 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:58:25,650 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:58:35,695 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:58:35,756 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:58:45,801 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:58:55,846 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:59:05,891 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:59:15,935 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:59:15,998 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:59:26,043 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:59:36,094 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:59:46,140 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:59:56,184 | WARNING | LLM SDK error attempt=5: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 06:59:56,247 | WARNING | LLM SDK error attempt=1: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 07:00:06,291 | WARNING | LLM SDK error attempt=2: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 07:00:16,335 | WARNING | LLM SDK error attempt=3: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-11-26 07:00:26,380 | WARNING | LLM SDK error attempt=4: Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 4096. This model's maximum context length is 32768 tokens and your request has 28783 input tokens (4096 > 32768 - 28783). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
